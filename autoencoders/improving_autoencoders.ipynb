{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization, ReLU, Add, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(foldername, target_size=(64, 64)):\n",
    "    images = []\n",
    "    for filename in os.listdir(foldername):\n",
    "        img = cv.imread(os.path.join(foldername, filename), cv.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv.resize(img, target_size)\n",
    "            images.append(img)\n",
    "    return images\n",
    "    \n",
    "def PSNR(original, compressed):\n",
    "    original = tf.cast(original, tf.float32)\n",
    "    compressed = tf.cast(compressed, tf.float32)\n",
    "    mse = tf.reduce_mean(tf.square(original - compressed))\n",
    "    if mse == 0:\n",
    "        return tf.constant(100.0)  # perfect match\n",
    "    psnr = 20 * tf.math.log(1.0 / tf.sqrt(mse)) / tf.math.log(10.0)  # max_pixel is 1.0 after normalization\n",
    "    return psnr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building blocks\n",
    "input_img = Input(shape=(64, 64, 1))\n",
    "\n",
    "def res_block(input_tensor):\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Add()([input_tensor, x])\n",
    "    out = ReLU()(out)\n",
    "    return out\n",
    "\n",
    "def inception_block(input_tensor):\n",
    "    a = Conv2D(32, (1, 1), padding='same', activation='relu')(input_tensor)\n",
    "    b = Conv2D(64, (3, 3), padding='same', activation='relu')(input_tensor)\n",
    "    c = Conv2D(64, (5, 5), padding='same', activation='relu')(input_tensor)\n",
    "    d = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input_tensor)\n",
    "    d = Conv2D(32, (1, 1), padding='same', activation='relu')(d)\n",
    "    output = Concatenate(axis=-1)([a, b, c, d])\n",
    "    return output\n",
    "\n",
    "def encode(input_tensor):\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_tensor)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    return encoded\n",
    "\n",
    "def decode(input_tensor):\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_tensor)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    return decoded\n",
    "\n",
    "def res_forward(input_tensor):\n",
    "    encoded = encode(input_tensor)\n",
    "    res = res_block(encoded)\n",
    "    decoded = decode(res)\n",
    "    return decoded\n",
    "\n",
    "def inception_forward(input_tensor):\n",
    "    encoded = encode(input_tensor)\n",
    "    inception = inception_block(encoded)\n",
    "    decoded = decode(inception)\n",
    "    return decoded\n",
    "\n",
    "autoencoder_res = Model(input_img, res_forward(input_img))\n",
    "autoencoder_res.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder_inception = Model(input_img, inception_forward(input_img))\n",
    "autoencoder_inception.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images_from_folder(\"pneumonia\")\n",
    "images = np.array(images).astype('float32') / 255.0\n",
    "images = np.expand_dims(images, axis=-1)\n",
    "\n",
    "# Noisy datasets\n",
    "def add_poisson_noise(imgs, lam):\n",
    "    noisy = np.random.poisson(imgs * lam) / lam\n",
    "    noisy = np.clip(noisy, 0., 1.)\n",
    "    return noisy.astype('float32')\n",
    "\n",
    "lam25 = add_poisson_noise(images, 25)\n",
    "lam50 = add_poisson_noise(images, 50)\n",
    "lam75 = add_poisson_noise(images, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split datasets\n",
    "x_train, x_test = train_test_split(images, test_size=0.2, random_state=42)\n",
    "train25, test25 = train_test_split(lam25, test_size=0.2, random_state=42)\n",
    "train50, test50 = train_test_split(lam50, test_size=0.2, random_state=42)\n",
    "train75, test75 = train_test_split(lam75, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - loss: 0.6108 - val_loss: 0.6807\n",
      "Epoch 2/5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 2s/step - loss: 0.5788 - val_loss: 0.6704\n",
      "Epoch 3/5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 949ms/step - loss: 0.5769 - val_loss: 0.6622\n",
      "Epoch 4/5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - loss: 0.5762 - val_loss: 0.6524\n",
      "Epoch 5/5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1s/step - loss: 0.5749 - val_loss: 0.6446\n",
      "Epoch 1/5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 1s/step - loss: 0.6555 - val_loss: 0.5953\n",
      "Epoch 2/5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - loss: 0.5873 - val_loss: 0.5840\n",
      "Epoch 3/5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1s/step - loss: 0.5789 - val_loss: 0.5810\n",
      "Epoch 4/5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - loss: 0.5781 - val_loss: 0.5797\n",
      "Epoch 5/5\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - loss: 0.5761 - val_loss: 0.5790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3554d1e50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train models\n",
    "autoencoder_res.fit(train25, x_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(test25, x_test))\n",
    "\n",
    "autoencoder_inception.fit(train25, x_train,\n",
    "                         epochs=5,\n",
    "                         batch_size=128,\n",
    "                         shuffle=True,\n",
    "                         validation_data=(test25, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step\n",
      "Mean PSNR for Residual Skip Connections model on lambda=25: 15.3600235\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step\n",
      "Mean PSNR for Inception Network model on lambda=25: 24.415216\n"
     ]
    }
   ],
   "source": [
    "# Evaluate PSNR for residual autoencoder\n",
    "denoised_res_lam25 = autoencoder_res.predict(test25)\n",
    "psnr_res_lam25 = np.mean([PSNR(gt, pred).numpy() for gt, pred in zip(x_test, denoised_res_lam25)])\n",
    "print(\"Mean PSNR for Residual Skip Connections model on lambda=25:\", psnr_res_lam25)\n",
    "\n",
    "# Evaluate PSNR for inception autoencoder\n",
    "denoised_inception_lam25 = autoencoder_inception.predict(test25)\n",
    "psnr_inception_lam25 = np.mean([PSNR(gt, pred).numpy() for gt, pred in zip(x_test, denoised_inception_lam25)])\n",
    "print(\"Mean PSNR for Inception Network model on lambda=25:\", psnr_inception_lam25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Which approach performed better for this particular task of denoising?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the PSNR (Peak Signal-to-Noise Ratio) values obtained for the lam25 noisy images:\n",
    "- PSNR for Residual Skip Connections model on lam25: 20.177284\n",
    "- PSNR for Inception Network model on lam25: 20.04568\n",
    "\n",
    "It can be observed that the Residual Skip Connections model achieved a higher PSNR value as compared to the Inception Network model. Since PSNR measures the quality of the denoised image wherein a higher value indicates better denoising, it can be concluded that Residual Blocks performed better for denoising the lambda 25 noisy images in this particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is the intuition behind the improved model which made it ideal for this particular task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improved model with Residual skip connections involve bypassing one or more intermediate layers by directly adding the input of a layer to its output. This allows the network to learn residual mappings, capturing the difference between the input and the output of a layer. By facilitating the flow of gradients during training and preserving useful information from earlier layers, residual skip connections enable the training of deeper networks and help mitigate the vanishing gradient problem. This mechanism contributes to improved learning efficiency and model performance in various tasks, including image denoising.\n",
    "\n",
    "On the other hand, the improved model with Inception blocks is ideal for denoising tasks due to its ability to capture features at multiple scales simultaneously. By incorporating convolutional filters of varying sizes and parallel pathways for feature extraction, the model can effectively preserve important details while removing noise. This adaptive combination of information and hierarchical representation learning enables the model to capture complex patterns present in noisy images, contributing to enhanced denoising performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
